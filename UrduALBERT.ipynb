{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training an Urdu Language Model using ALBERT Architecture","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll explore the process of training a language model for the Urdu language using the ALBERT architecture. We will use the Hugging Face Transformers library to perform tokenization, model configuration, and training.","metadata":{}},{"cell_type":"markdown","source":"## Imports and Setup","metadata":{}},{"cell_type":"code","source":"# Pakcages\n!pip install transformers sentencepiece datasets\n\n# libraries\nimport sentencepiece as sp\nfrom datasets import load_dataset\nfrom transformers import (\n    AlbertForMaskedLM,\n    AlbertConfig,\n    AlbertTokenizer,\n    DataCollatorForLanguageModeling,\n    LineByLineTextDataset,\n    Trainer,\n    TrainingArguments\n)\n\nimport os\n\nimport huggingface_hub \nhuggingface_hub.login(\"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\")\nfrom huggingface_hub import HfFolder\n\nimport wandb\nwandb.login(key=\"d920e57c9f860eba9eba5bc0a71b6a5aa91761b4\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading and Preprocessing Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"anuragshas/ur_opus100_processed\")\n\ntrain_dataset = dataset['train']\nvalidation_dataset = dataset['train'].train_test_split(test_size=0.1)['test']","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:21:08.126185Z","iopub.execute_input":"2023-08-27T07:21:08.127080Z","iopub.status.idle":"2023-08-27T07:21:12.397135Z","shell.execute_reply.started":"2023-08-27T07:21:08.127035Z","shell.execute_reply":"2023-08-27T07:21:12.396124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_dataset['text']\nvalidation_data = validation_dataset['text']","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:26:08.201026Z","iopub.execute_input":"2023-08-27T07:26:08.201427Z","iopub.status.idle":"2023-08-27T07:26:11.050880Z","shell.execute_reply.started":"2023-08-27T07:26:08.201394Z","shell.execute_reply":"2023-08-27T07:26:11.049740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('ur.txt', 'w', encoding='utf-8') as f:\n    for item in train_data:\n        f.write(item + '\\n')\n        \nwith open('val_ur.txt', 'w', encoding='utf-8') as f:\n    for item in validation_data:\n        f.write(item + '\\n')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:26:11.063752Z","iopub.execute_input":"2023-08-27T07:26:11.064105Z","iopub.status.idle":"2023-08-27T07:26:12.397630Z","shell.execute_reply.started":"2023-08-27T07:26:11.064077Z","shell.execute_reply":"2023-08-27T07:26:12.396597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization \nUsing SentencePiece to create a custom tokenizer for our Urdu dataset.","metadata":{}},{"cell_type":"code","source":"sp.SentencePieceTrainer.train(input=\"/kaggle/working/ur.txt\",model_prefix='spiece', vocab_size=23319)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:26:12.399240Z","iopub.execute_input":"2023-08-27T07:26:12.399606Z","iopub.status.idle":"2023-08-27T07:27:37.005135Z","shell.execute_reply.started":"2023-08-27T07:26:12.399571Z","shell.execute_reply":"2023-08-27T07:27:37.004041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sp.SentencePieceTrainer.train(input=\"/kaggle/working/val_ur.txt\", model_prefix='spiece_val', vocab_size=14354)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:28:07.596209Z","iopub.execute_input":"2023-08-27T07:28:07.596567Z","iopub.status.idle":"2023-08-27T07:28:15.311325Z","shell.execute_reply.started":"2023-08-27T07:28:07.596538Z","shell.execute_reply":"2023-08-27T07:28:15.307471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.mkdir('Urdu_Model')\nos.rename('spiece.model','Urdu_Model/spiece.model')\nos.rename('spiece.vocab','Urdu_Model/spiece.vocab')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:28:22.785045Z","iopub.execute_input":"2023-08-27T07:28:22.785430Z","iopub.status.idle":"2023-08-27T07:28:22.843934Z","shell.execute_reply.started":"2023-08-27T07:28:22.785394Z","shell.execute_reply":"2023-08-27T07:28:22.841381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urdu_tokenizer = AlbertTokenizer.from_pretrained('Urdu_Model')\n\nurdu_tokenizer.save_pretrained('Urdu_Model')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:28:28.758089Z","iopub.execute_input":"2023-08-27T07:28:28.758499Z","iopub.status.idle":"2023-08-27T07:28:28.846503Z","shell.execute_reply.started":"2023-08-27T07:28:28.758465Z","shell.execute_reply":"2023-08-27T07:28:28.845364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Initialization and Config","metadata":{}},{"cell_type":"code","source":"config=AlbertConfig.from_pretrained('albert-large-v2')\n\nconfig.save_pretrained('Urdu_Model')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:28:32.690969Z","iopub.execute_input":"2023-08-27T07:28:32.691350Z","iopub.status.idle":"2023-08-27T07:28:32.781442Z","shell.execute_reply.started":"2023-08-27T07:28:32.691321Z","shell.execute_reply":"2023-08-27T07:28:32.780332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urdu_model = AlbertForMaskedLM(config=config)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:28:32.926513Z","iopub.execute_input":"2023-08-27T07:28:32.927004Z","iopub.status.idle":"2023-08-27T07:28:33.284562Z","shell.execute_reply.started":"2023-08-27T07:28:32.926952Z","shell.execute_reply":"2023-08-27T07:28:33.283539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataLoader and Data Collator","metadata":{}},{"cell_type":"code","source":"train_line_by_line_dataset = LineByLineTextDataset(\n    tokenizer=urdu_tokenizer,\n    file_path=\"/kaggle/working/ur.txt\",\n    block_size=256, #block_size means number of tokens in a sequence.\n)\n\nvalidation_line_by_line_dataset = LineByLineTextDataset(\n    tokenizer=urdu_tokenizer,\n    file_path=\"/kaggle/working/val_ur.txt\",\n    block_size=256,\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=urdu_tokenizer,mlm=True, mlm_probability=0.15)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T07:38:57.594326Z","iopub.execute_input":"2023-08-27T07:38:57.594697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Config","metadata":{}},{"cell_type":"code","source":"repository_id = \"mwz/UrduALBERT\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=repository_id,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,  \n    evaluation_strategy=\"steps\",  # Evaluate at every logging_steps\n    eval_steps=500,  # Evaluate every 200 steps\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=1000,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    report_to=\"tensorboard\",\n    push_to_hub=True,\n    hub_strategy=\"every_save\",\n    hub_model_id=repository_id,\n    hub_token=HfFolder.get_token(),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=urdu_model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_line_by_line_dataset,\n    eval_dataset=validation_line_by_line_dataset,  \n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T19:07:22.872216Z","iopub.execute_input":"2023-08-26T19:07:22.872607Z","iopub.status.idle":"2023-08-26T19:24:01.590367Z","shell.execute_reply.started":"2023-08-26T19:07:22.872574Z","shell.execute_reply":"2023-08-26T19:24:01.588474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pushing it to [Hub](https://huggingface.co/mwz/UrduALBERT)","metadata":{}},{"cell_type":"code","source":"trainer.create_model_card()\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T19:24:01.591347Z","iopub.status.idle":"2023-08-26T19:24:01.591932Z","shell.execute_reply.started":"2023-08-26T19:24:01.591687Z","shell.execute_reply":"2023-08-26T19:24:01.591716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}